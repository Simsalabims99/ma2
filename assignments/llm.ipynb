{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "import pandas as pd\n",
    "from decouple import config \n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm\n",
    "\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to WatsonX.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading api key from .env file\n",
    "api_key = config('wx_api_key')\n",
    "\n",
    "# Accessing the API\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com/\",\n",
    "    api_key = api_key\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials,\n",
    "    project_id=\"ce1ea911-fc95-453c-9e8b-02ff019d04e8\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-24T08:15:05.052Z',\n",
       " 'results': [{'generated_text': 'Mix the ingredients together in a bowl. Pour the batter into a cake pan. Bake for 30 minutes',\n",
       "   'generated_token_count': 20,\n",
       "   'input_token_count': 7,\n",
       "   'stop_reason': 'max_tokens'}],\n",
       " 'system': {'warnings': [{'message': \"The value of 'parameters.max_new_tokens' for this model was set to value 20\",\n",
       "    'id': 'unspecified_max_new_tokens',\n",
       "    'additional_properties': {'limit': 0,\n",
       "     'new_value': 20,\n",
       "     'parameter': 'parameters.max_new_tokens',\n",
       "     'value': 0}}]}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How do I make a cake?\"\n",
    "generated_response = model.generate(prompt)\n",
    "\n",
    "generated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Fears for T N pension after talks Unions repre...      2\n",
      "1  The Race is On: Second Private Team Sets Launc...      3\n",
      "2  Ky. Company Wins Grant to Study Peptides (AP) ...      3\n",
      "3  Prediction Unit Helps Forecast Wildfires (AP) ...      3\n",
      "4  Calif. Aims to Limit Farm-Related Smog (AP) AP...      3\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "\n",
    "train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])\n",
    "\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "# del test\n",
    "\n",
    "test_df.shape, # train_df.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters for WatsonX AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "# Collection of models to train on\n",
    "model_ids = [\n",
    "    \"ibm/granite-13b-instruct-v2\",\n",
    "    \"meta-llama/llama-3-405b-instruct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You task is to classify news stories into one of four pre-fixed categories\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "EXAMPLES:\n",
    "TEXT: \"Apple just launched a new AI-powered MacBook.\"  \n",
    "Category: Sci/Tech  \n",
    "\n",
    "TEXT: \"The stock market crashed after the latest interest rate hike.\"  \n",
    "Category: Business  \n",
    "\n",
    "TEXT: \"A major earthquake has struck Japan, causing widespread damage.\"  \n",
    "Category: World  \n",
    "\n",
    "TEXT: \"Manchester United won their latest match with a last-minute goal.\"  \n",
    "Category: Sports  \n",
    "\n",
    "Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Business\n",
      "- Sci/Tech\n",
      "- Sports\n",
      "- World\n"
     ]
    }
   ],
   "source": [
    "# Print categories\n",
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())\n",
    "print(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [04:10<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "# Array to store predictions for each model\n",
    "predictions_ibm = []\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=model_ids[0],  # IBM Model id\n",
    "    params=PARAMS\n",
    ")\n",
    "\n",
    "# Train on all models in model ids\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "\n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the correct list of predictions\n",
    "    predictions_ibm.append(prediction)\n",
    " \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [05:38<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "# Array to store predictions for each model\n",
    "predictions_llama = []\n",
    "\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=model_ids[1],  # Llama model id\n",
    "    params=PARAMS\n",
    ")\n",
    "\n",
    "# Train on all models in model ids\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "\n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the correct list of predictions\n",
    "    predictions_llama.append(prediction)\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM Prediction Evaluation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.54      0.71      0.61       190\n",
      "    Sci/Tech       1.00      0.02      0.03       190\n",
      "      Sports       0.39      0.94      0.55       190\n",
      "       World       0.75      0.23      0.35       190\n",
      "\n",
      "    accuracy                           0.47       760\n",
      "   macro avg       0.67      0.47      0.39       760\n",
      "weighted avg       0.67      0.47      0.39       760\n",
      "\n",
      "Llama Prediction Evaluation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.77      0.93      0.84       190\n",
      "    Sci/Tech       0.91      0.72      0.81       190\n",
      "      Sports       0.97      0.97      0.97       190\n",
      "       World       0.95      0.85      0.90       190\n",
      "         ```       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       760\n",
      "   macro avg       0.72      0.69      0.70       760\n",
      "weighted avg       0.90      0.87      0.88       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"IBM Prediction Evaluation: \")\n",
    "print(classification_report(test_df.label, predictions_ibm))\n",
    "print(\"Llama Prediction Evaluation: \")\n",
    "print(classification_report(test_df.label, predictions_llama))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "#### Model Reflections\n",
    "Significant improvements are seen in the Llama model compared to the baseline IBM Granite model with an overall accuracy improvement from 0.47 to 0.87.\n",
    "When training the models the Llama model was significantly more sensitive towards the prompting technique, as numorous iterations was needed to find the best prompting technique to ensure that the model did not create new categories that best suited the text. Long and descriptive prompts had a lower performance than the resulting prompt which is clear and has few instructions to follow.\n",
    "\n",
    "Still, it can be seen from the Classification Report that the Llama model struggled to fit all texts into one of the four pre-defined categories.\n",
    "\n",
    "#### Prompt Technique Reflections\n",
    "Few-Shot Learning technique was added to the prompt to enhance performance, which significantly enhanced the model's ability to only utilise the existing categories for classification.\n",
    "\n",
    "- By adding examples for each category, the models seemed to perform better at classifying the texts and provided clear guidelines for the task to avoid creation of new categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiml-ma2)",
   "language": "python",
   "name": "aiml25-ma2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
